{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6889ab8d-0a22-4bba-8dc0-d5b03d23c844"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import PIL.Image\n",
    "import PIL.ImageSequence\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "from IPython.display import display, clear_output\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "import math\n",
    "import glob\n",
    "import csv\n",
    "from functools import partial\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "import colorsys\n",
    "import requests\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d32e7195-2958-409b-b272-7284e737ec20"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Load network snapshot\n",
    "##\n",
    "\n",
    "#input_sg_name = \"2019-02-09-stylegan-danbooru2017-faces-network-snapshot-007841.pkl\"\n",
    "\n",
    "# From https://mega.nz/#!vOgj1QoD!GD3E37BroNnZaIR_nic2zVxBtKfAqlvbEC8uBK8-4co\n",
    "input_sg_name = \"2019-02-18-stylegan-faces-network-02041-011095.pkl\"\n",
    "\n",
    "tflib.init_tf()\n",
    "\n",
    "# Load pre-trained network.\n",
    "with open(input_sg_name, 'rb') as f:\n",
    "    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.    \n",
    "    _G, _D, Gs = pickle.load(f)\n",
    "        \n",
    "# Print network details.\n",
    "Gs.print_layers()\n",
    "_D.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c1d6d694-1896-460e-8fe7-3f29ca6a7c97"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Build things on top for encoding\n",
    "# Unfortunately this works only Kind Of Okay\n",
    "# Based on https://github.com/Puzer/stylegan\n",
    "##\n",
    "def create_stub(name, batch_size):\n",
    "    return tf.constant(0, dtype='float32', shape=(batch_size, 0))\n",
    "\n",
    "dlatent_avg = tf.get_default_session().run(Gs.own_vars[\"dlatent_avg\"])\n",
    "def create_variable_for_generator(name, batch_size):\n",
    "    truncation_psi_encode = 0.7\n",
    "    layer_idx = np.arange(16)[np.newaxis, :, np.newaxis]\n",
    "    ones = np.ones(layer_idx.shape, dtype=np.float32)\n",
    "    coefs = tf.where(layer_idx < 8, truncation_psi_encode * ones, ones)\n",
    "    dlatent_variable = tf.get_variable(\n",
    "        'learnable_dlatents', \n",
    "        shape=(1, 16, 512), \n",
    "        dtype='float32', \n",
    "        initializer=tf.initializers.zeros()\n",
    "    )\n",
    "    dlatent_variable_trunc = tflib.lerp(dlatent_avg, dlatent_variable, coefs)\n",
    "    return dlatent_variable_trunc\n",
    "\n",
    "# Generation-from-disentangled-latents part\n",
    "initial_dlatents = np.zeros((1, 16, 512))\n",
    "Gs.components.synthesis.run(\n",
    "    initial_dlatents,\n",
    "    randomize_noise = True, # Turns out this should not be off ever for trying to lean dlatents, who knew\n",
    "    minibatch_size = 1,\n",
    "    custom_inputs = [\n",
    "        partial(create_variable_for_generator, batch_size=1),\n",
    "        partial(create_stub, batch_size = 1)],\n",
    "    structure = 'fixed'\n",
    ")\n",
    "\n",
    "dlatent_variable = next(v for v in tf.global_variables() if 'learnable_dlatents' in v.name)\n",
    "generator_output = tf.get_default_graph().get_tensor_by_name('G_synthesis_1/_Run/G_synthesis/images_out:0')\n",
    "generated_image = tflib.convert_images_to_uint8(generator_output, nchw_to_nhwc=True, uint8_cast=False)\n",
    "generated_image_uint8 = tf.saturate_cast(generated_image, tf.uint8)\n",
    "\n",
    "# Loss part\n",
    "vgg16 = VGG16(include_top=False, input_shape=(512, 512, 3))\n",
    "perceptual_model = keras.Model(vgg16.input, vgg16.layers[9].output)\n",
    "generated_img_features = perceptual_model(preprocess_input(generated_image, mode=\"tf\"))\n",
    "ref_img = tf.get_variable(\n",
    "    'ref_img', \n",
    "    shape = generated_image.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "ref_img_features = tf.get_variable(\n",
    "    'ref_img_features', \n",
    "    shape = generated_img_features.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "tf.get_default_session().run([ref_img.initializer, ref_img_features.initializer])\n",
    "basic_loss = tf.losses.mean_squared_error(ref_img, generated_image)\n",
    "perceptual_loss = tf.losses.mean_squared_error(ref_img_features, generated_img_features)\n",
    "\n",
    "_D.run(np.zeros((1, 3, 512, 512)), None, custom_inputs = [\n",
    "    lambda x: generator_output,\n",
    "    partial(create_stub, batch_size = 1),\n",
    "])\n",
    "discriminator_output = tf.get_default_graph().get_tensor_by_name('D/_Run/D/scores_out:0')\n",
    "\n",
    "# Attempt at making encoding better: Bias towards mean (\"truncation loss\", essentially)\n",
    "dlatent_avg_full = dlatent_avg.reshape(-1, 512).repeat(16, axis = 0).reshape(-1, 16, 512)\n",
    "input_loss = tf.losses.mean_squared_error(dlatent_variable, dlatent_avg_full)\n",
    "combined_loss = input_loss + perceptual_loss\n",
    "\n",
    "# We have a discriminator network, why not use it?\n",
    "discriminator_loss = tf.nn.softplus(-discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e060a3ca-26b3-4147-977d-8897eab5aa4a"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient descend in latent space to something that is similar to the input image\n",
    "def encode_image(image, iterations = 1024, learning_rate = 0.1, reset_dlatents = True, custom_initial_dlatents = None):\n",
    "    # Get session\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    # Gradient descent initial state\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(perceptual_loss, var_list=[[dlatent_variable]])\n",
    "    if reset_dlatents == True:\n",
    "        if not custom_initial_dlatents is None:\n",
    "            sess.run(tf.assign(dlatent_variable, custom_initial_dlatents.reshape(-1, 16, 512)))\n",
    "        else:\n",
    "            sess.run(tf.assign(dlatent_variable, initial_dlatents))\n",
    "    \n",
    "    # Generate and set reference image features\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float32\")), [image])))\n",
    "    image_features = perceptual_model.predict_on_batch(preprocess_input(ref_image_data, mode=\"tf\"))  \n",
    "    sess.run(tf.assign(ref_img_features, image_features))\n",
    "    \n",
    "    # Run\n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, perceptual_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "    \n",
    "    # Generate image that actually goes with these dlatents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Same as above but start with given dlatents and use plain MSE loss instead of vgg16\n",
    "def finetune_image(dlatents, image, iterations = 32, learning_rate = 0.0001):\n",
    "    # Get session and assign initial dlatents\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    \n",
    "    # Set reference image\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float64\")), [image])))\n",
    "    sess.run(tf.assign(ref_img, ref_image_data))    \n",
    "    \n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(basic_loss, var_list=[[dlatent_variable]])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, basic_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "\n",
    "    # Generate image that actually goes with these latents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Tune image in the direction of being considered more likely by the discriminator\n",
    "def tune_with_discriminator(dlatents, iterations = 32, learning_rate = 1.0):\n",
    "    # Get session and assign initial dlatents\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    \n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(discriminator_loss, var_list=[[dlatent_variable]])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, basic_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "    \n",
    "    return sess.run(dlatent_variable)[0]\n",
    "\n",
    "# We have to do truncation ourselves, since we're not using the combined network\n",
    "def truncate(dlatents, truncation_psi, maxlayer = 8):\n",
    "    dlatent_avg = tf.get_default_session().run(Gs.own_vars[\"dlatent_avg\"])\n",
    "    layer_idx = np.arange(16)[np.newaxis, :, np.newaxis]\n",
    "    ones = np.ones(layer_idx.shape, dtype=np.float32)\n",
    "    coefs = tf.where(layer_idx < maxlayer, truncation_psi * ones, ones)\n",
    "    return tf.get_default_session().run(tflib.lerp(dlatent_avg, dlatents, coefs))\n",
    "\n",
    "# Generate image with disentangled latents as input\n",
    "def generate_images_from_dlatents(dlatents, truncation_psi = 1.0, randomize_noise = True):\n",
    "    if not truncation_psi is None:\n",
    "        dlatents_trunc = truncate(dlatents, truncation_psi)\n",
    "    else:\n",
    "        dlatents_trunc = dlatents\n",
    "        \n",
    "    # Run the network\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    result_image = Gs.components.synthesis.run(\n",
    "        dlatents_trunc.reshape((-1, 16, 512)),\n",
    "        randomize_noise = randomize_noise,\n",
    "        minibatch_size = 1,\n",
    "        output_transform=fmt\n",
    "    )[0]\n",
    "    return result_image\n",
    "\n",
    "# Sequence of learning steps while reducing lr followed by finetune\n",
    "def encode_and_tune(image, iters_per_step = 1024):\n",
    "    initial_latents = np.random.randn(1, Gs.input_shape[1])\n",
    "    initial_dlatents = Gs.components.mapping.run(initial_latents, None)[0]\n",
    "    dlatents_gen, image_gen = encode_image(image, iterations = iters_per_step, learning_rate = 100.0, custom_initial_dlatents = initial_dlatents)\n",
    "    dlatents_gen2, image_gen2 = encode_image(image, iterations = iters_per_step, learning_rate = 10.0, reset_dlatents = False)\n",
    "    dlatents_gen3, image_gen3 = encode_image(image, iterations = iters_per_step, learning_rate = 1.0, reset_dlatents = False)\n",
    "    dlatents_gen4, image_gen4 = encode_image(image, iterations = iters_per_step, learning_rate = 0.1, reset_dlatents = False)\n",
    "    dlatents_gen5, image_gen5 = encode_image(image, iterations = iters_per_step, learning_rate = 0.01, reset_dlatents = False)\n",
    "    dlatents_gen6, image_gen6 = encode_image(image, iterations = iters_per_step, learning_rate = 0.001, reset_dlatents = False)\n",
    "    dlatents_gen7, image_gen7 = finetune_image(dlatents_gen5, image, iterations = 128)\n",
    "    return dlatents_gen7, image_gen7, dlatents_gen6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "68ca55b8-efe3-49c7-8e64-62f1923d93ea"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Interactive modification!\n",
    "hair_eyes_only = False # Set to true for fewer tags\n",
    "lock_updates = False\n",
    "def modify_and_sample(psi, truncate_pre, truncate_post, **kwargs):\n",
    "    global lock_updates\n",
    "    if lock_updates == True:\n",
    "        return\n",
    "    \n",
    "    if truncate_pre == True:\n",
    "        dlatents_mod = truncate(copy.deepcopy(dlatents_gen), psi)\n",
    "    else:\n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        \n",
    "    for tag in kwargs:\n",
    "        dlatents_mod += tag_directions[tag] * kwargs[tag]\n",
    "    value_widgets[\"psi\"].value = str(round(psi, 2))\n",
    "    \n",
    "    for tag in kwargs:\n",
    "        tag_value = round((np.dot(dlatents_mod.flatten(), tag_directions[tag].flatten()) / tag_len[tag]) - kwargs[tag], 2)\n",
    "        value_widgets[tag].value = str(kwargs[tag]) + \" | \" + str(tag_value)\n",
    "    \n",
    "    display_psi = None\n",
    "    if truncate_post == True:\n",
    "        display_psi = psi\n",
    "    display(PIL.Image.fromarray(generate_images_from_dlatents(dlatents_mod, truncation_psi = display_psi), 'RGB'))\n",
    "\n",
    "# Load up tags and tag directions\n",
    "with open(\"tag_dirs.pkl\", 'rb') as f:\n",
    "    tag_directions = pickle.load(f)\n",
    "    \n",
    "tag_len = {}\n",
    "for tag in tag_directions:\n",
    "    tag_len[tag] = np.linalg.norm(tag_directions[tag].flatten())\n",
    "    \n",
    "mod_latents = np.load(\"mod_latents.npy\")\n",
    "dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "    \n",
    "if hair_eyes_only:\n",
    "    modify_tags = [tag for tag in tag_directions if \"_hair\" in tag or \"_eyes\" in tag or \"_mouth\" in tag]\n",
    "    modify_tags.append(\"realistic\")\n",
    "else:\n",
    "    with open(\"tags_use.pkl\", \"rb\") as f:\n",
    "        modify_tags = pickle.load(f)\n",
    "\n",
    "# Build UI\n",
    "psi_slider = widgets.FloatSlider(min = 0.0, max = 1.0, step = 0.01, value = 0.7, continuous_update = False, readout = False)\n",
    "tag_widgets = {}\n",
    "for tag in modify_tags:\n",
    "    tag_widgets[tag] = widgets.FloatSlider(min = -2.0, max = 2.0, step = 0.01, continuous_update = False, readout = False)\n",
    "all_widgets = []\n",
    "\n",
    "sorted_widgets = sorted(tag_widgets.items(), key = lambda x: x[0])\n",
    "sorted_widgets = [(\"psi\", psi_slider)] + sorted_widgets\n",
    "value_widgets = {}\n",
    "for widget in sorted_widgets:\n",
    "    label_widget = widgets.Label(widget[0])\n",
    "    label_widget.layout.width = \"140px\"\n",
    "    \n",
    "    value_widget = widgets.Label(\"0.0+100.0\")\n",
    "    value_widget.layout.width = \"150px\"\n",
    "    value_widgets[widget[0]] = value_widget\n",
    "    \n",
    "    tag_hbox = widgets.HBox([label_widget, widget[1], value_widget])\n",
    "    tag_hbox.layout.width = \"320px\"\n",
    "    \n",
    "    all_widgets.append(tag_hbox)\n",
    "\n",
    "refresh = widgets.Button(description=\"New Sample\")\n",
    "modify = widgets.Button(description=\"Mutate\")\n",
    "reset = widgets.Button(description=\"Reset Tags\")\n",
    "\n",
    "def new_sample(b):\n",
    "    global mod_latents\n",
    "    global dlatents_gen\n",
    "    mod_latents = np.random.randn(1, Gs.input_shape[1])\n",
    "    dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0] \n",
    "    psi_slider.value += 0.00000000001 # idk how to properly\n",
    "    \n",
    "def mutate(b):\n",
    "    global mod_latents\n",
    "    global dlatents_gen\n",
    "    mod_latents_add = np.random.randn(1, Gs.input_shape[1]) * 0.2\n",
    "    mod_latents += mod_latents_add\n",
    "    dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "    psi_slider.value += 0.00000000001\n",
    "\n",
    "def reset_tags(b):\n",
    "    global lock_updates\n",
    "    lock_updates = True\n",
    "    for widget in real_tag_widgets.values():\n",
    "        widget.value = 0.0\n",
    "    lock_updates = False\n",
    "    psi_slider.value += 0.00000000001\n",
    "\n",
    "real_tag_widgets = copy.copy(tag_widgets)\n",
    "\n",
    "truncate_pre = widgets.ToggleButton(value=True, description='Truncate Pre')\n",
    "truncate_post = widgets.ToggleButton(value=False, description='Truncate Post')\n",
    "refresh.on_click(new_sample)\n",
    "modify.on_click(mutate)\n",
    "reset.on_click(reset_tags)\n",
    "\n",
    "for button in [refresh, modify, truncate_pre, truncate_post, reset]:\n",
    "    button.layout.width = \"120px\"\n",
    "\n",
    "ui = widgets.Box(all_widgets + [refresh, modify, truncate_pre, truncate_post, reset])\n",
    "tag_widgets[\"psi\"] = psi_slider\n",
    "\n",
    "ui.layout.flex_flow = 'row wrap'\n",
    "ui.layout.display = 'inline-flex'\n",
    "tag_widgets[\"truncate_pre\"] = truncate_pre\n",
    "tag_widgets[\"truncate_post\"] = truncate_post\n",
    "out = widgets.interactive_output(modify_and_sample, tag_widgets)\n",
    "\n",
    "# Lets go! (best used in Presentation Mode)\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbpresent": {
   "slides": {
    "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655": {
     "id": "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655",
     "prev": null,
     "regions": {
      "234da276-ee07-47ef-b598-c4ba844db0b8": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": -0.0008051529790660225,
        "y": 0
       },
       "content": {
        "cell": "68ca55b8-efe3-49c7-8e64-62f1923d93ea",
        "part": "outputs"
       },
       "id": "234da276-ee07-47ef-b598-c4ba844db0b8",
       "x": 0
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
