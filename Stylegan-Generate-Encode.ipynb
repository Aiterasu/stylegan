{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import PIL.Image\n",
    "import PIL.ImageSequence\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import IPython.display\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "import math\n",
    "import glob\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Load network snapshot\n",
    "##\n",
    "\n",
    "#input_sg_name = \"2019-02-09-stylegan-danbooru2017-faces-network-snapshot-007841.pkl\"\n",
    "input_sg_name = \"2019-02-18-stylegan-faces-network-02041-011095.pkl\"\n",
    "\n",
    "tflib.init_tf()\n",
    "\n",
    "# Load pre-trained network.\n",
    "with open(input_sg_name, 'rb') as f:\n",
    "    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.    \n",
    "    _G, _D, Gs = pickle.load(f)\n",
    "        \n",
    "# Print network details.\n",
    "Gs.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Build things on top for encoding\n",
    "# Based on https://github.com/Puzer/stylegan\n",
    "##\n",
    "def create_stub(name, batch_size):\n",
    "    return tf.constant(0, dtype='float32', shape=(batch_size, 0))\n",
    "\n",
    "def create_variable_for_generator(name, batch_size):\n",
    "    return tf.get_variable('learnable_dlatents',\n",
    "                           shape=(batch_size, 16, 512),\n",
    "                           dtype='float32',\n",
    "                           initializer=tf.initializers.zeros())\n",
    "\n",
    "initial_dlatents = np.zeros((1, 16, 512))\n",
    "Gs.components.synthesis.run(\n",
    "    initial_dlatents,\n",
    "    randomize_noise = True, # Turns out this should not be off ever for trying to lean dlatents, who knew\n",
    "    minibatch_size = 1,\n",
    "    custom_inputs = [\n",
    "        partial(create_variable_for_generator, batch_size=1),\n",
    "        partial(create_stub, batch_size = 1)],\n",
    "    structure = 'fixed'\n",
    ")\n",
    "\n",
    "# Generation-from-disentangled-latents part\n",
    "dlatent_variable = next(v for v in tf.global_variables() if 'learnable_dlatents' in v.name)\n",
    "generator_output = tf.get_default_graph().get_tensor_by_name('G_synthesis_1/_Run/G_synthesis/images_out:0')\n",
    "generated_image = tflib.convert_images_to_uint8(generator_output, nchw_to_nhwc=True, uint8_cast=False)\n",
    "generated_image_uint8 = tf.saturate_cast(generated_image, tf.uint8)\n",
    "\n",
    "# Loss part\n",
    "vgg16 = VGG16(include_top=False, input_shape=(512, 512, 3))\n",
    "perceptual_model = keras.Model(vgg16.input, vgg16.layers[9].output)\n",
    "generated_img_features = perceptual_model(preprocess_input(generated_image, mode=\"tf\"))\n",
    "ref_img = tf.get_variable(\n",
    "    'ref_img', \n",
    "    shape = generated_image.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "ref_img_features = tf.get_variable(\n",
    "    'ref_img_features', \n",
    "    shape = generated_img_features.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "tf.get_default_session().run([ref_img.initializer, ref_img_features.initializer])\n",
    "basic_loss = tf.losses.mean_squared_error(ref_img, generated_image)\n",
    "perceptual_loss = tf.losses.mean_squared_error(ref_img_features, generated_img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descend in latent space to something that is similar to the input image\n",
    "def encode_image(image, iterations = 1024, learning_rate = 0.1, reset_dlatents = True):\n",
    "    # Get session\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    # Gradient descent initial state\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(perceptual_loss, var_list=[[dlatent_variable]])\n",
    "    if reset_dlatents == True:\n",
    "        sess.run(tf.assign(dlatent_variable, initial_dlatents))\n",
    "    \n",
    "    # Generate and set reference image features\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float32\")), [image])))\n",
    "    image_features = perceptual_model.predict_on_batch(preprocess_input(ref_image_data, mode=\"tf\"))  \n",
    "    sess.run(tf.assign(ref_img_features, image_features))\n",
    "    \n",
    "    # Run\n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, perceptual_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "            \n",
    "    # Generate image that actually goes with these dlatents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Same as above but start with given dlatents and use plain MSE loss instead of vgg16\n",
    "def finetune_image(dlatents, image, iterations = 32, learning_rate = 0.0001):\n",
    "    # Get session and assign initial dlatents\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    \n",
    "    # Set reference image\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float64\")), [image])))\n",
    "    sess.run(tf.assign(ref_img, ref_image_data))    \n",
    "    \n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(basic_loss, var_list=[[dlatent_variable]])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, basic_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "            \n",
    "    # Generate image that actually goes with these latents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Generate image with disentangled latents as input\n",
    "def generate_images_from_dlatents(dlatents):\n",
    "    tf.get_default_session().run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    return tf.get_default_session().run(generated_image_uint8)[0]\n",
    "\n",
    "# Sequence of learning steps while reducing lr followed by finetune\n",
    "def encode_and_tune(image):\n",
    "    dlatents_gen, image_gen = encode_image(image, iterations = 1024, learning_rate = 100.0)\n",
    "    dlatents_gen2, image_gen2 = encode_image(image, iterations = 1024, learning_rate = 10.0, reset_dlatents = False)\n",
    "    dlatents_gen3, image_gen3 = encode_image(image, iterations = 1024, learning_rate = 1.0, reset_dlatents = False)\n",
    "    dlatents_gen4, image_gen4 = encode_image(image, iterations = 1024, learning_rate = 0.1, reset_dlatents = False)\n",
    "    dlatents_gen5, image_gen5 = encode_image(image, iterations = 1024, learning_rate = 0.01, reset_dlatents = False)\n",
    "    dlatents_gen6, image_gen6 = finetune_image(dlatents_gen5, image, iterations = 64)\n",
    "    return dlatents_gen6, image_gen6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# 1. Just generate a neat interpolation video\n",
    "##\n",
    "# Pick latent vectors\n",
    "#rnd = np.random.RandomState(5)\n",
    "rnd = np.random\n",
    "latents_a = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_b = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_c = rnd.randn(1, Gs.input_shape[1])\n",
    "\n",
    "if os.path.exists(\"latents.npy\"):\n",
    "    latents_a, latents_b, latents_c = np.load(\"latents.npy\")\n",
    "np.save(\"latents.npy\", np.array([latents_a, latents_b, latents_c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Ellipse around a point but probably a circle since it's 512 dimensions\"\n",
    "def circ_generator(latents_interpolate):\n",
    "    radius = 40.0\n",
    "\n",
    "    latents_axis_x = (latents_a - latents_b).flatten() / la.norm(latents_a - latents_b)\n",
    "    latents_axis_y = (latents_a - latents_c).flatten() / la.norm(latents_a - latents_c)\n",
    "\n",
    "    latents_x = math.sin(math.pi * 2.0 * latents_interpolate) * radius\n",
    "    latents_y = math.cos(math.pi * 2.0 * latents_interpolate) * radius\n",
    "\n",
    "    latents = latents_a + latents_x * latents_axis_x + latents_y * latents_axis_y\n",
    "    return latents\n",
    "\n",
    "# Generate images from a list of latents\n",
    "def generate_from_latents(latent_list):\n",
    "    array_list = []\n",
    "    image_list = []\n",
    "    for latents in latent_list:\n",
    "        # Generate image.\n",
    "        fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "        images = Gs.run(latents, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)\n",
    "        array_list.append(images[0])\n",
    "        image_list.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
    "        \n",
    "    return array_list, image_list\n",
    "\n",
    "def mse(x, y):\n",
    "    return (np.square(x - y)).mean()\n",
    "\n",
    "# Generate from a latent generator, keeping MSE between frames constant\n",
    "def generate_from_generator_adaptive(gen_func):\n",
    "    max_step = 1.0\n",
    "    current_pos = 0.0\n",
    "    \n",
    "    change_min = 10.0\n",
    "    change_max = 11.0\n",
    "    \n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    \n",
    "    current_latent = gen_func(current_pos)\n",
    "    current_image = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "    array_list = []\n",
    "    \n",
    "    while(current_pos < 1.0):\n",
    "        array_list.append(current_image)\n",
    "        \n",
    "        lower = current_pos\n",
    "        upper = current_pos + max_step\n",
    "        current_pos = (upper + lower) / 2.0\n",
    "        \n",
    "        current_latent = gen_func(current_pos)\n",
    "        current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "        current_mse = mse(array_list[-1], current_image)\n",
    "        \n",
    "        while current_mse < change_min or current_mse > change_max:\n",
    "            if current_mse < change_min:\n",
    "                lower = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "            \n",
    "            if current_mse > change_max:\n",
    "                upper = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "                \n",
    "            \n",
    "            current_latent = gen_func(current_pos)\n",
    "            current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "            current_mse = mse(array_list[-1], current_image)\n",
    "        print(current_pos, current_mse)\n",
    "        \n",
    "    return array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_list, _ = generate_from_latents(latent_list)\n",
    "array_list = generate_from_generator_adaptive(circ_generator)\n",
    "clip = moviepy.editor.ImageSequenceClip(array_list, fps=60)\n",
    "clip.ipython_display()\n",
    "#clip.write_videofile(\"out.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# 2. Encoding\n",
    "##\n",
    "\n",
    "# Load and cut and scale a bunch of data from the animefaces dataset\n",
    "img_files = []\n",
    "hair_cols = []\n",
    "for in_dir in glob.glob(\"../../stylegan/animeface-character-dataset/thumb/*\"):\n",
    "    if not os.path.exists(in_dir + \"/color.csv\"):\n",
    "        continue\n",
    "    with open(in_dir + \"/color.csv\", 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            img_files.append(in_dir + \"/\" + row[0])\n",
    "            hair_cols.append([row[1], row[2], row[3]])\n",
    "\n",
    "image_arrays = []\n",
    "for img_file in img_files[0:4]:\n",
    "    image_data = PIL.Image.open(img_file)\n",
    "    image_size = min(image_data.width, image_data.height)\n",
    "    image_data = image_data.crop((0, 0, image_size, image_size))\n",
    "    image_data = image_data.resize((512, 512), PIL.Image.BILINEAR)\n",
    "    image_array = np.array(image_data)\n",
    "    image_arrays.append(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlatents_gen, image_gen = encode_and_tune(image_arrays[0])\n",
    "im = PIL.Image.new('RGB', (1024, 512))\n",
    "im.paste(PIL.Image.fromarray(image_arrays[0], 'RGB'), (0, 0))\n",
    "im.paste(PIL.Image.fromarray(image_gen, 'RGB'), (512, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_dlatents = []\n",
    "generated_images = []\n",
    "for image_array in image_arrays:\n",
    "    # Unfortunately very slow. \n",
    "    # Generated examples, but still takes a long time to get it quite right\n",
    "    dlatents_gen, image_gen = encode_image(image_array)\n",
    "    \n",
    "    # Seems to help, maybe vgg16 is not ideal for anime faces or maybe sth else is wrong\n",
    "    dlatents_gen, image_gen = finetune_image(dlatents_gen, image_array)\n",
    "    \n",
    "    generated_dlatents.append(dlatents_gen)\n",
    "    generated_images.append(image_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encode a generated image\n",
    "generated_ref = generate_from_latents([latents_a])[0][0]\n",
    "dlatents_gen, image_gen = encode_and_tune(generated_ref)\n",
    "im = PIL.Image.new('RGB', (1024, 512))\n",
    "im.paste(PIL.Image.fromarray(generated_ref, 'RGB'), (0, 0))\n",
    "im.paste(PIL.Image.fromarray(image_gen, 'RGB'), (512, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
